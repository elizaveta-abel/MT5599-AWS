{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c725d6e-359e-4549-a144-86d1a33f839d",
   "metadata": {},
   "source": [
    "# Week 8 Progress Report\n",
    "\n",
    "The purpose of this report is to record the progress on the MT5599 Project.\n",
    "\n",
    "The main milestones of the project so far are:\n",
    "\n",
    "- Tweets from Argentinian residents and visitors from 2016 & 2020 have been collected\n",
    "- Dataset from PAHO WHO of dengue cases from 2016 to 2021 has been found\n",
    "- There is a second dataset of all PAHO dengue cases from a similar period exists and I have it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cc561-8428-4623-aa81-6778a17f0c6a",
   "metadata": {},
   "source": [
    "## Tweets from Argentina\n",
    "\n",
    "### Collecting Tweets\n",
    "\n",
    "The original proposed method for collecting tweets was:\n",
    "\n",
    "1. Choose 2-3 months from each epidemic year (2016, 2020) and collect all tweets that are geotagged \"Argentina\"\n",
    "2. Identify those handles whose user location is Argentina for each 2016, 2020 (residents)\n",
    "3. Collect all their tweets from the affected year\n",
    "\n",
    "However, it was found that there were very handles collected this way (a few hundred), which was not at all representative of Argentina's roughly 5.9 million Twitter active users.\n",
    "\n",
    "This is why the new method became:\n",
    "\n",
    "1. Collect all tweets from near Argentina during selected periods (2015-10-01 to 2016-11-01) and (2019-06-01 and 2020-07-01) _See `scraper_handles.py` for this script_\n",
    "2. If person had an Argentinian city/town/province in their user information, they were labelled as a resident. If they had a tweet with country tag AR (Argentina), they were labelled as a visitor. There is overlap, however in both years the number of residents is significantly lower than the number of visitors. The duplicates were removed in later analysis. _See `extracting_handles.py` and `simpler_getting_argentinian_residents.ipynb`_\n",
    "3. All the tweets from the above collected handles were scraped during the selected periods. This yielded roughly ___ for 2016 and ___ for 2020. _See `EDA.ipynb` for insights_\n",
    "\n",
    "### Cleaning Tweets\n",
    "\n",
    "Many tweets have emojis, hashtags, links, etc. These were cleaned using `cleaning_tweets.py`.\n",
    "\n",
    "### Processing Tweets\n",
    "\n",
    "The purpose of the tweets is to tell us:\n",
    "\n",
    "- where people were at all times of the year\n",
    "- what their purpose of travel was\n",
    "- whether they mentioned dengue or other relevant topics\n",
    "\n",
    "To tell us where people were at all times of the year, we can use the coordinates data (which not all t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5ead7-1392-4201-ac9a-cb0ba4d7de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
