{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ce254e-82d5-4216-99d1-c25aa3492ddd",
   "metadata": {},
   "source": [
    "# Geotagged Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de8662f-0057-4a95-accb-8281753ab4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting epiweeks\n",
      "  Using cached epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n",
      "Installing collected packages: epiweeks\n",
      "Successfully installed epiweeks-2.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting kaleido\n",
      "  Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "Installing collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install epiweeks\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036ea77b-39dd-4e59-9020-3ed261785cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool # multithreading\n",
    "from epiweeks import Week\n",
    "\n",
    "# granularity\n",
    "def granularity_helper(row):\n",
    "    \n",
    "    s = row[1][\"gmaps_address\"]\n",
    "    \n",
    "    week = Week.fromdate(row[1][\"DateTime\"], system=\"iso\")\n",
    "    row[1][\"epi_week\"] = week.cdcformat()\n",
    "    return row[1]\n",
    "\n",
    "\n",
    "\n",
    "def granularity(df):\n",
    "    \n",
    "    pool = Pool(processes=round(len(df.index)/10000))\n",
    "\n",
    "    result_arr = []\n",
    "    \n",
    "\n",
    "    for result in tqdm.tqdm(pool.imap_unordered(granularity_helper, df.iterrows()),\n",
    "                            total=len(df.index)):\n",
    "        result_arr.append(result)\n",
    "    \n",
    "\n",
    "    df = pd.concat(result_arr, axis=1).transpose().sort_index()\n",
    "\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60be6f-0b41-4752-ab52-c9f304c0e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604983/604983 [03:59<00:00, 2524.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648397/648397 [04:13<00:00, 2562.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 639553/639553 [04:17<00:00, 2486.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626847/626847 [04:10<00:00, 2501.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 571553/571553 [03:51<00:00, 2468.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593821/593821 [04:02<00:00, 2450.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608854/608854 [04:08<00:00, 2446.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638423/638423 [04:23<00:00, 2427.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 644039/644039 [04:24<00:00, 2437.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687154/687154 [04:45<00:00, 2405.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653473/653473 [04:33<00:00, 2390.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620660/620660 [04:21<00:00, 2375.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656571/656571 [04:34<00:00, 2395.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690920/690920 [04:53<00:00, 2355.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539424/539424 [03:47<00:00, 2371.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619273/619273 [04:22<00:00, 2359.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622727/622727 [04:23<00:00, 2366.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 590872/590872 [04:15<00:00, 2316.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527191/527191 [03:47<00:00, 2314.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517063/517063 [03:50<00:00, 2243.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 671038/671038 [04:55<00:00, 2268.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579348/579348 [04:17<00:00, 2250.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495563/495563 [03:41<00:00, 2236.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599744/599744 [04:26<00:00, 2250.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517836/517836 [04:01<00:00, 2140.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 451967/451967 [03:31<00:00, 2140.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 555531/555531 [04:15<00:00, 2177.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627400/627400 [04:46<00:00, 2188.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472943/472943 [03:36<00:00, 2181.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477523/477523 [03:43<00:00, 2137.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489456/489456 [04:00<00:00, 2037.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515424/515424 [03:56<00:00, 2180.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502995/502995 [03:59<00:00, 2098.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483265/483265 [03:47<00:00, 2123.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474041/474041 [03:58<00:00, 1983.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 467857/467857 [03:53<00:00, 2003.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474298/474298 [03:55<00:00, 2016.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490894/490894 [03:59<00:00, 2050.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454082/454082 [03:52<00:00, 1951.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 467849/467849 [04:05<00:00, 1908.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429715/429715 [03:40<00:00, 1952.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443398/443398 [03:44<00:00, 1972.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438321/438321 [03:45<00:00, 1939.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483156/483156 [04:13<00:00, 1902.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583514/583514 [05:03<00:00, 1922.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624761/624761 [05:19<00:00, 1957.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460649/460649 [04:20<00:00, 1771.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  91\n"
     ]
    }
   ],
   "source": [
    "################ GEOTAGGED ONLY ##########################\n",
    "\n",
    "# get distances column from all datasets\n",
    "# first determine if there are any duplicate tweet ids in there\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "for i in range(44, 112):\n",
    "    print(\"starting \", i)\n",
    "    df = pd.read_feather(\"s3://mt5599/final/processed_tweets_\" + str(i) + \".feather\")\n",
    "\n",
    "    # keep tweets with no distances and NER mention\n",
    "    df = df[pd.notnull(df.place_full_name)]\n",
    "    df = granularity(df)\n",
    "    df = df.reset_index()\n",
    "    df.to_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9bf71-84d9-4b5c-a2c0-8ea5f3a5eeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f10113-474a-4fa2-8011-cc6bc5eb64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining into one dataset\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    df = pd.read_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(drop=True).reset_index()\n",
    "df.to_feather(\"s3://mt5599/final/geotagged.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2978574-fd7a-4273-8c8e-5e0bb0977a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    df = pd.read_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(drop=True).reset_index()\n",
    "df.to_feather(\"s3://mt5599/final/geotagged.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdb808-d041-44cd-944c-392365bb04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather(\"s3://mt5599/final/geotagged.feather\")\n",
    "print(\"read\")\n",
    "df = df.drop_duplicates(\"id\").drop([\"level_0\", \"index\"], axis=1)\n",
    "print(\"cleaning\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d750666-3cb8-4bb9-b719-d1140ed082a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating top 20 most geotagged countries and their frequencies\n",
    "\n",
    "loc_vc = df.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged countries,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c06716-a652-48ee-9dea-566abc7873f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For each user, the countries they geotagged over the course of the relevant time period were found.\")\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "user_df = df.groupby(['username'])[\"place_country\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_df.countries.str.len()\n",
    "\n",
    "print()\n",
    "print(\"The mean number of geotagged countries was \", np.mean(countries_per_user),\n",
    "     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_df.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows their breakdown.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bce112-631c-4ec5-8c2f-b39a930d311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ARGENTINA - tweets\n",
    "\n",
    "ar = df[df.place_country_code == \"AR\"]\n",
    "\n",
    "loc_vc = ar.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8a2c3-13c7-4c85-9cbf-8bd8e4bde21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNITED STATES - tweets\n",
    "\n",
    "us = df[df.place_country_code == \"US\"]\n",
    "\n",
    "loc_vc = us.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc7272-7b6f-4d77-8e63-4b859536b295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BRAZIL - tweets\n",
    "\n",
    "br = df[df.place_country_code == \"BR\"]\n",
    "\n",
    "loc_vc = br.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b1846-9562-4839-8d75-4d0c65d45c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ARGENTINA GEOTAGS - by user\n",
    "\n",
    "ar = df[df.place_country_code == \"AR\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = ar.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9ab54-2dca-4dfe-b7fb-1f7ee0f10397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"There were a total of \", ar.shape[0],\" geotagged tweets from Argentina.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93943bfe-2e12-4279-9d2d-e4f5e2e71b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNITED STATES GEOTAGS - by user\n",
    "\n",
    "us = df[df.place_country_code == \"US\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = us.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6a9fe-074e-4d8b-86b0-6cc036ea40d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BRAZIL GEOTAGS - by user\n",
    "\n",
    "br = df[df.place_country_code == \"BR\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = br.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f14903-67c8-47c5-8ac2-6afa77070842",
   "metadata": {},
   "source": [
    "## Epi Weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b434b4-3ce5-4967-94e2-52dcbaee64c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - tweets\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_20 = df.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#top_20 = top_20.loc[,0:20]\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "top_20\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = df[df.place_country.isin(top_20)]\n",
    "loc_p = loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(df.place_country.unique(), df.epi_week.unique())), columns=['place_country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"place_country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "#only_loc_p = df_loc[df_loc.place_country.isin(top_20)]\n",
    "#only_loc_p = only_loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "# plot pie chart\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"place_country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Tweets\",\n",
    "                     \"place_country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdce71-c59e-4b98-b4b0-d7e276d39204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - users\n",
    "\n",
    "import itertools\n",
    "\n",
    "cepi = df.drop_duplicates([\"username\", \"place_country\", \"epi_week\"])\n",
    "cepi = cepi.groupby(['place_country', \"epi_week\"])[\"username\"].count().reset_index(name='counts')\n",
    "cepi = cepi.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_20 = cepi.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = cepi[cepi.place_country.isin(top_20)]\n",
    "#loc_p = loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "#loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(cepi.place_country.unique(), df.epi_week.unique())), columns=['place_country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"place_country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"place_country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Users in Country\",\n",
    "                     \"place_country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94181c31-c63a-4bf8-98af-08fa4d4799ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lower value counts of Place Object?"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
