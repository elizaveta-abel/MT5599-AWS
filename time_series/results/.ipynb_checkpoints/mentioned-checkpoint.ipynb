{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7299700-6c23-430c-840e-e66731f29be0",
   "metadata": {},
   "source": [
    "# Mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd3965f-a5eb-4aae-8a08-b76b9fe7ae13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6431ab88-8034-4fdc-9f76-db7c54d84cb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport tqdm\\n\\nfor i in tqdm.tqdm(range(0, 112)):\\n    df = pd.read_feather(\"s3://mt5599/final/processed_tweets_\" + str(i) + \".feather\")\\n\\n    # keep tweets with no distances and NER mention\\n    mdf_loc_only = df[pd.isnull(df.coordinates_longitude) & (df.ner_type == \"LOC\")].reset_index()\\n    mdf_loc_only.to_feather(\"s3://mt5599/dissertation/mentioned_loc_only_df_\" + str(i) + \".feather\" )\\n    mdf_loc_only_duplicates = mdf_loc_only[mdf_loc_only.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\\n    if mdf_loc_only_duplicates.shape[0] > 0:\\n        mdf_loc_only_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_only_df_\" + str(i) + \".feather\" )\\n    # new subset with only not tweet duplicates\\n    mdf_loc_only_noduplicates = mdf_loc_only.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\\n    mdf_loc_only_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_only_df_\" + str(i) + \".feather\" )\\n    \\n    \\n    # filter tweets with only LOC mention\\n    mdf_loc = df[df.ner_type == \"LOC\"].reset_index()\\n    mdf_loc.to_feather(\"s3://mt5599/dissertation/mentioned_loc_df_\" + str(i) + \".feather\" )\\n    # new subset with only tweet duplicates\\n    mdf_loc_duplicates = mdf_loc[mdf_loc.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\\n    if mdf_loc_duplicates.shape[0] > 0:\\n        mdf_loc_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_df_\" + str(i) + \".feather\" )\\n    # new subset with only not tweet duplicates\\n    mdf_loc_noduplicates = mdf_loc.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\\n    mdf_loc_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_df_\" + str(i) + \".feather\" )\\n    \\n    # filter tweets with only ORG mention\\n    mdf_org = df[df.ner_type == \"ORG\"].reset_index()\\n    mdf_org.to_feather(\"s3://mt5599/dissertation/mentioned_org_df_\" + str(i) + \".feather\" )\\n    # new subset with only tweet duplicates\\n    mdf_org_duplicates = mdf_org[mdf_org.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\\n    if mdf_org_duplicates.shape[0] > 0:\\n        mdf_org_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_org_df_\" + str(i) + \".feather\" )\\n    # new subset with only not tweet duplicates\\n    mdf_org_noduplicates = mdf_org.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\\n    mdf_org_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_org_df_\" + str(i) + \".feather\" )\\n    \\n        \\n    # filter tweets with only PER mention\\n    mdf_per = df[df.ner_type == \"PER\"].reset_index()\\n    mdf_per.to_feather(\"s3://mt5599/dissertation/mentioned_per_df_\" + str(i) + \".feather\" )\\n    # new subset with only tweet duplicates\\n    mdf_per_duplicates = mdf_per[mdf_per.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\\n    if mdf_per_duplicates.shape[0] > 0:\\n        mdf_per_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_per_df_\" + str(i) + \".feather\" )\\n    # new subset with only not tweet duplicates\\n    mdf_per_noduplicates = mdf_per.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\\n    mdf_per_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_per_df_\" + str(i) + \".feather\" )\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ MENTIONED ONLY ##########################\n",
    "\n",
    "# get distances column from all datasets\n",
    "# first determine if there are any duplicate tweet ids in there\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    df = pd.read_feather(\"s3://mt5599/final/processed_tweets_\" + str(i) + \".feather\")\n",
    "\n",
    "    # keep tweets with no distances and NER mention\n",
    "    mdf_loc_only = df[pd.isnull(df.coordinates_longitude) & (df.ner_type == \"LOC\")].reset_index()\n",
    "    mdf_loc_only.to_feather(\"s3://mt5599/dissertation/mentioned_loc_only_df_\" + str(i) + \".feather\" )\n",
    "    mdf_loc_only_duplicates = mdf_loc_only[mdf_loc_only.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\n",
    "    if mdf_loc_only_duplicates.shape[0] > 0:\n",
    "        mdf_loc_only_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_only_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only not tweet duplicates\n",
    "    mdf_loc_only_noduplicates = mdf_loc_only.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\n",
    "    mdf_loc_only_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_only_df_\" + str(i) + \".feather\" )\n",
    "    \n",
    "    \n",
    "    # filter tweets with only LOC mention\n",
    "    mdf_loc = df[df.ner_type == \"LOC\"].reset_index()\n",
    "    mdf_loc.to_feather(\"s3://mt5599/dissertation/mentioned_loc_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only tweet duplicates\n",
    "    mdf_loc_duplicates = mdf_loc[mdf_loc.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\n",
    "    if mdf_loc_duplicates.shape[0] > 0:\n",
    "        mdf_loc_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only not tweet duplicates\n",
    "    mdf_loc_noduplicates = mdf_loc.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\n",
    "    mdf_loc_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_df_\" + str(i) + \".feather\" )\n",
    "    \n",
    "    # filter tweets with only ORG mention\n",
    "    mdf_org = df[df.ner_type == \"ORG\"].reset_index()\n",
    "    mdf_org.to_feather(\"s3://mt5599/dissertation/mentioned_org_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only tweet duplicates\n",
    "    mdf_org_duplicates = mdf_org[mdf_org.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\n",
    "    if mdf_org_duplicates.shape[0] > 0:\n",
    "        mdf_org_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_org_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only not tweet duplicates\n",
    "    mdf_org_noduplicates = mdf_org.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\n",
    "    mdf_org_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_org_df_\" + str(i) + \".feather\" )\n",
    "    \n",
    "        \n",
    "    # filter tweets with only PER mention\n",
    "    mdf_per = df[df.ner_type == \"PER\"].reset_index()\n",
    "    mdf_per.to_feather(\"s3://mt5599/dissertation/mentioned_per_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only tweet duplicates\n",
    "    mdf_per_duplicates = mdf_per[mdf_per.duplicated(subset = [\"id\"], keep=False)].drop(\"index\", axis=1).reset_index()\n",
    "    if mdf_per_duplicates.shape[0] > 0:\n",
    "        mdf_per_duplicates.to_feather(\"s3://mt5599/dissertation/duplicates_mentioned_per_df_\" + str(i) + \".feather\" )\n",
    "    # new subset with only not tweet duplicates\n",
    "    mdf_per_noduplicates = mdf_per.drop_duplicates(subset=\"id\", keep=False).drop(\"index\", axis=1).reset_index()\n",
    "    mdf_per_noduplicates.to_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_per_df_\" + str(i) + \".feather\" )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1733c22e-af4b-4eb8-9c38-2540e3fc2c00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmdfs_loc = []\\nmdfs_loc_only = []\\nmdfs_org = []\\nmdfs_per = []\\nmdfs_loc_duplicates = []\\nmdfs_loc_only_duplicates = []\\nmdfs_org_duplicates = []\\nmdfs_per_duplicates = []\\nmdfs_loc_noduplicates = []\\nmdfs_loc_only_noduplicates = []\\nmdfs_org_noduplicates = []\\nmdfs_per_noduplicates = []\\n\\nfor i in tqdm.tqdm(range(0, 112)):\\n    mdf_loc = pd.read_feather(\"s3://mt5599/dissertation/mentioned_loc_df_\" + str(i) + \".feather\")\\n    mdf_loc_only = pd.read_feather(\"s3://mt5599/dissertation/mentioned_loc_only_df_\" + str(i) + \".feather\")\\n    mdf_org = pd.read_feather(\"s3://mt5599/dissertation/mentioned_org_df_\" + str(i) + \".feather\")\\n    mdf_per = pd.read_feather(\"s3://mt5599/dissertation/mentioned_per_df_\" + str(i) + \".feather\")\\n    mdf_loc_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_df_\" + str(i) + \".feather\")\\n    mdf_loc_only_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_only_df_\" + str(i) + \".feather\")\\n    mdf_org_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_org_df_\" + str(i) + \".feather\")\\n    mdf_per_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_per_df_\" + str(i) + \".feather\")\\n    mdf_loc_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_df_\" + str(i) + \".feather\")\\n    mdf_loc_only_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_only_df_\" + str(i) + \".feather\")\\n    mdf_org_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_org_df_\" + str(i) + \".feather\")\\n    mdf_per_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_per_df_\" + str(i) + \".feather\")\\n    \\n    mdfs_loc.append(mdf_loc)\\n    mdfs_loc_only.append(mdf_loc_only)\\n    mdfs_org.append(mdf_org)\\n    mdfs_per.append(mdf_per)\\n    mdfs_loc_duplicates.append(mdf_loc_duplicates)\\n    mdfs_loc_only_duplicates.append(mdf_loc_only_duplicates)\\n    mdfs_org_duplicates.append(mdf_org_duplicates)\\n    mdfs_per_duplicates.append(mdf_per_duplicates)\\n    mdfs_loc_noduplicates.append(mdf_loc_noduplicates)\\n    mdfs_loc_only_noduplicates.append(mdf_loc_only_noduplicates)\\n    mdfs_org_noduplicates.append(mdf_org_noduplicates)\\n    mdfs_per_noduplicates.append(mdf_per_noduplicates)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining into one dataset\n",
    "\"\"\"\n",
    "mdfs_loc = []\n",
    "mdfs_loc_only = []\n",
    "mdfs_org = []\n",
    "mdfs_per = []\n",
    "mdfs_loc_duplicates = []\n",
    "mdfs_loc_only_duplicates = []\n",
    "mdfs_org_duplicates = []\n",
    "mdfs_per_duplicates = []\n",
    "mdfs_loc_noduplicates = []\n",
    "mdfs_loc_only_noduplicates = []\n",
    "mdfs_org_noduplicates = []\n",
    "mdfs_per_noduplicates = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    mdf_loc = pd.read_feather(\"s3://mt5599/dissertation/mentioned_loc_df_\" + str(i) + \".feather\")\n",
    "    mdf_loc_only = pd.read_feather(\"s3://mt5599/dissertation/mentioned_loc_only_df_\" + str(i) + \".feather\")\n",
    "    mdf_org = pd.read_feather(\"s3://mt5599/dissertation/mentioned_org_df_\" + str(i) + \".feather\")\n",
    "    mdf_per = pd.read_feather(\"s3://mt5599/dissertation/mentioned_per_df_\" + str(i) + \".feather\")\n",
    "    mdf_loc_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_df_\" + str(i) + \".feather\")\n",
    "    mdf_loc_only_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_loc_only_df_\" + str(i) + \".feather\")\n",
    "    mdf_org_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_org_df_\" + str(i) + \".feather\")\n",
    "    mdf_per_duplicates = pd.read_feather(\"s3://mt5599/dissertation/duplicates_mentioned_per_df_\" + str(i) + \".feather\")\n",
    "    mdf_loc_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_df_\" + str(i) + \".feather\")\n",
    "    mdf_loc_only_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_loc_only_df_\" + str(i) + \".feather\")\n",
    "    mdf_org_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_org_df_\" + str(i) + \".feather\")\n",
    "    mdf_per_noduplicates = pd.read_feather(\"s3://mt5599/dissertation/noduplicates_mentioned_per_df_\" + str(i) + \".feather\")\n",
    "    \n",
    "    mdfs_loc.append(mdf_loc)\n",
    "    mdfs_loc_only.append(mdf_loc_only)\n",
    "    mdfs_org.append(mdf_org)\n",
    "    mdfs_per.append(mdf_per)\n",
    "    mdfs_loc_duplicates.append(mdf_loc_duplicates)\n",
    "    mdfs_loc_only_duplicates.append(mdf_loc_only_duplicates)\n",
    "    mdfs_org_duplicates.append(mdf_org_duplicates)\n",
    "    mdfs_per_duplicates.append(mdf_per_duplicates)\n",
    "    mdfs_loc_noduplicates.append(mdf_loc_noduplicates)\n",
    "    mdfs_loc_only_noduplicates.append(mdf_loc_only_noduplicates)\n",
    "    mdfs_org_noduplicates.append(mdf_org_noduplicates)\n",
    "    mdfs_per_noduplicates.append(mdf_per_noduplicates)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc83bf0-75d4-4b63-aec6-f24531becb70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nmdf_loc = pd.concat(mdfs_loc, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc.to_feather(\"s3://mt5599/final/mentioned_loc.feather\")\\n\\nmdf_loc_only = pd.concat(mdfs_loc_only, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc_only.to_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\\n\\nmdf_org = pd.concat(mdfs_org, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_org.to_feather(\"s3://mt5599/final/mentioned_org.feather\")\\n\\nmdf_per = pd.concat(mdfs_per, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_per.to_feather(\"s3://mt5599/final/mentioned_per.feather\")\\n\\nmdf_loc_duplicates = pd.concat(mdfs_loc_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc_duplicates.to_feather(\"s3://mt5599/final/mentioned_loc_duplicates.feather\")\\n\\nmdf_loc_only_duplicates = pd.concat(mdfs_loc_only_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc_only_duplicates.to_feather(\"s3://mt5599/final/mentioned_loc_only_duplicates.feather\")\\n\\nmdf_org_duplicates = pd.concat(mdfs_org_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_org_duplicates.to_feather(\"s3://mt5599/final/mentioned_org_duplicates.feather\")\\n\\nmdf_per_duplicates = pd.concat(mdfs_per_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_per_duplicates.to_feather(\"s3://mt5599/final/mentioned_per_duplicates.feather\")\\n\\nmdf_loc_noduplicates = pd.concat(mdfs_loc_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc_noduplicates.to_feather(\"s3://mt5599/final/mentioned_loc_noduplicates.feather\")\\n\\nmdf_loc_only_noduplicates = pd.concat(mdfs_loc_only_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_loc_only_noduplicates.to_feather(\"s3://mt5599/final/mentioned_loc_only_noduplicates.feather\")\\n\\nmdf_org_noduplicates = pd.concat(mdfs_org_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_org_noduplicates.to_feather(\"s3://mt5599/final/mentioned_org_noduplicates.feather\")\\n\\nmdf_per_noduplicates = pd.concat(mdfs_per_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\\nmdf_per_noduplicates.to_feather(\"s3://mt5599/final/mentioned_per_noduplicates.feather\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "mdf_loc = pd.concat(mdfs_loc, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc.to_feather(\"s3://mt5599/final/mentioned_loc.feather\")\n",
    "\n",
    "mdf_loc_only = pd.concat(mdfs_loc_only, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc_only.to_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\n",
    "\n",
    "mdf_org = pd.concat(mdfs_org, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_org.to_feather(\"s3://mt5599/final/mentioned_org.feather\")\n",
    "\n",
    "mdf_per = pd.concat(mdfs_per, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_per.to_feather(\"s3://mt5599/final/mentioned_per.feather\")\n",
    "\n",
    "mdf_loc_duplicates = pd.concat(mdfs_loc_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc_duplicates.to_feather(\"s3://mt5599/final/mentioned_loc_duplicates.feather\")\n",
    "\n",
    "mdf_loc_only_duplicates = pd.concat(mdfs_loc_only_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc_only_duplicates.to_feather(\"s3://mt5599/final/mentioned_loc_only_duplicates.feather\")\n",
    "\n",
    "mdf_org_duplicates = pd.concat(mdfs_org_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_org_duplicates.to_feather(\"s3://mt5599/final/mentioned_org_duplicates.feather\")\n",
    "\n",
    "mdf_per_duplicates = pd.concat(mdfs_per_duplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_per_duplicates.to_feather(\"s3://mt5599/final/mentioned_per_duplicates.feather\")\n",
    "\n",
    "mdf_loc_noduplicates = pd.concat(mdfs_loc_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc_noduplicates.to_feather(\"s3://mt5599/final/mentioned_loc_noduplicates.feather\")\n",
    "\n",
    "mdf_loc_only_noduplicates = pd.concat(mdfs_loc_only_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_loc_only_noduplicates.to_feather(\"s3://mt5599/final/mentioned_loc_only_noduplicates.feather\")\n",
    "\n",
    "mdf_org_noduplicates = pd.concat(mdfs_org_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_org_noduplicates.to_feather(\"s3://mt5599/final/mentioned_org_noduplicates.feather\")\n",
    "\n",
    "mdf_per_noduplicates = pd.concat(mdfs_per_noduplicates, axis=0, ignore_index=True).reset_index(drop=True).drop(\"index\", axis=1)\n",
    "mdf_per_noduplicates.to_feather(\"s3://mt5599/final/mentioned_per_noduplicates.feather\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1232dfd0-ce37-4773-a90d-6a3b26784b06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nimport pandas as pd\\n\\nprint(\"mdf_loc\")\\n#mdf_loc = pd.read_feather(\"s3://mt5599/final/mentioned_loc.feather\")\\nprint(\"mdf_loc_only\")\\nmdf_loc_only = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"mdf_loc\")\n",
    "#mdf_loc = pd.read_feather(\"s3://mt5599/final/mentioned_loc.feather\")\n",
    "print(\"mdf_loc_only\")\n",
    "mdf_loc_only = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9dfab57-8519-4411-abcb-e86fad75bf3a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmdf_org = pd.read_feather(\"s3://mt5599/final/mentioned_org.feather\")\\nmdf_per = pd.read_feather(\"s3://mt5599/final/mentioned_per.feather\")\\nmdf_loc_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_duplicates.feather\")\\nmdf_loc_only_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only_duplicates.feather\")\\nmdf_org_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_org_duplicates.feather\")\\nmdf_per_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_per_duplicates.feather\")\\nmdf_loc_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_noduplicates.feather\")\\nmdf_loc_only_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only_noduplicates.feather\")\\nmdf_org_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_org_noduplicates.feather\")\\nmdf_per_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_per_noduplicates.feather\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mdf_org = pd.read_feather(\"s3://mt5599/final/mentioned_org.feather\")\n",
    "mdf_per = pd.read_feather(\"s3://mt5599/final/mentioned_per.feather\")\n",
    "mdf_loc_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_duplicates.feather\")\n",
    "mdf_loc_only_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only_duplicates.feather\")\n",
    "mdf_org_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_org_duplicates.feather\")\n",
    "mdf_per_duplicates = pd.read_feather(\"s3://mt5599/final/mentioned_per_duplicates.feather\")\n",
    "mdf_loc_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_noduplicates.feather\")\n",
    "mdf_loc_only_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only_noduplicates.feather\")\n",
    "mdf_org_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_org_noduplicates.feather\")\n",
    "mdf_per_noduplicates = pd.read_feather(\"s3://mt5599/final/mentioned_per_noduplicates.feather\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556171da-2404-4779-b222-9926cd0659ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: epiweeks in /opt/conda/lib/python3.7/site-packages (2.1.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kaleido in /opt/conda/lib/python3.7/site-packages (0.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install epiweeks\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2cb3f3-25d0-4fa7-9ae7-255baa060961",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tqdm\\nfrom multiprocessing import Pool # multithreading\\nfrom epiweeks import Week\\n\\n# granularity\\ndef granularity_helper(row):\\n    \\n    s = row[1][\"gmaps_address\"]\\n    \\n    # also add Epi Week processing\\n\\n    #my_date = date(2018, 12, 30)\\n    week = Week.fromdate(row[1][\"DateTime\"], system=\"iso\")\\n    row[1][\"epi_week\"] = week.cdcformat()\\n    \\n    if pd.notnull(s):\\n    \\n        split = s.split(\", \")\\n    \\n        row[1][\"split_address\"] = split\\n        \\n        row[1][\"country\"] = split[-1]\\n\\n\\n        if len(split) >=5:\\n            row[1][\"subcountry\"] = split[-3]\\n            if any(char.isdigit() for char in split[2]) or any(char.isdigit() for char in split[1]) or any(char.isdigit() for char in split[0]):\\n                row[1][\"granularity\"] = \"address\"\\n            else:\\n                row[1][\"granularity\"] = \"city/town\"\\n                row[1][\"city/town\"] = split[-3]\\n                \\n        elif len(split) ==4:\\n            row[1][\"subcountry\"] = split[-3]\\n            if any(char.isdigit() for char in split[1]) or any(char.isdigit() for char in split[0]):\\n                row[1][\"granularity\"] = \"address\"\\n            else:\\n                row[1][\"granularity\"] = \"city/town\"\\n                row[1][\"city/town\"] = split[-3]\\n                \\n        elif len(split) ==3:\\n            row[1][\"subcountry\"] = split[1]\\n            if any(char.isdigit() for char in split[0]):\\n                row[1][\"granularity\"] = \"address\"\\n            else:\\n                row[1][\"granularity\"] = \"city/town\"\\n                row[1][\"city/town\"] = split[-3]\\n                \\n        elif len(split) == 2:\\n            row[1][\"granularity\"] = \"subcountry\"\\n            row[1][\"subcountry\"] = split[-2]\\n            \\n        elif len(split) == 1:\\n            row[1][\"granularity\"] = \"country\"\\n            \\n\\n        #if len(split) >= 3:\\n        #    row[1][\"subcountry\"] = split[-3]\\n    \\n    return row[1]\\n\\n\\n\\n\\n\\ndef granularity(df):\\n    \\n    #df[\\'split_address\\'] = None\\n    \\n    pool = Pool(processes=round(len(df.index)/10000))\\n\\n    result_arr = []\\n    \\n    for result in tqdm.tqdm(pool.imap_unordered(granularity_helper, df.iterrows()),\\n                            total=len(df.index)):\\n        result_arr.append(result)\\n                \\n    df = pd.concat(result_arr, axis=1).transpose().sort_index()\\n                \\n    return df\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tqdm\n",
    "from multiprocessing import Pool # multithreading\n",
    "from epiweeks import Week\n",
    "\n",
    "# granularity\n",
    "def granularity_helper(row):\n",
    "    \n",
    "    s = row[1][\"gmaps_address\"]\n",
    "    \n",
    "    # also add Epi Week processing\n",
    "\n",
    "    #my_date = date(2018, 12, 30)\n",
    "    week = Week.fromdate(row[1][\"DateTime\"], system=\"iso\")\n",
    "    row[1][\"epi_week\"] = week.cdcformat()\n",
    "    \n",
    "    if pd.notnull(s):\n",
    "    \n",
    "        split = s.split(\", \")\n",
    "    \n",
    "        row[1][\"split_address\"] = split\n",
    "        \n",
    "        row[1][\"country\"] = split[-1]\n",
    "\n",
    "\n",
    "        if len(split) >=5:\n",
    "            row[1][\"subcountry\"] = split[-3]\n",
    "            if any(char.isdigit() for char in split[2]) or any(char.isdigit() for char in split[1]) or any(char.isdigit() for char in split[0]):\n",
    "                row[1][\"granularity\"] = \"address\"\n",
    "            else:\n",
    "                row[1][\"granularity\"] = \"city/town\"\n",
    "                row[1][\"city/town\"] = split[-3]\n",
    "                \n",
    "        elif len(split) ==4:\n",
    "            row[1][\"subcountry\"] = split[-3]\n",
    "            if any(char.isdigit() for char in split[1]) or any(char.isdigit() for char in split[0]):\n",
    "                row[1][\"granularity\"] = \"address\"\n",
    "            else:\n",
    "                row[1][\"granularity\"] = \"city/town\"\n",
    "                row[1][\"city/town\"] = split[-3]\n",
    "                \n",
    "        elif len(split) ==3:\n",
    "            row[1][\"subcountry\"] = split[1]\n",
    "            if any(char.isdigit() for char in split[0]):\n",
    "                row[1][\"granularity\"] = \"address\"\n",
    "            else:\n",
    "                row[1][\"granularity\"] = \"city/town\"\n",
    "                row[1][\"city/town\"] = split[-3]\n",
    "                \n",
    "        elif len(split) == 2:\n",
    "            row[1][\"granularity\"] = \"subcountry\"\n",
    "            row[1][\"subcountry\"] = split[-2]\n",
    "            \n",
    "        elif len(split) == 1:\n",
    "            row[1][\"granularity\"] = \"country\"\n",
    "            \n",
    "\n",
    "        #if len(split) >= 3:\n",
    "        #    row[1][\"subcountry\"] = split[-3]\n",
    "    \n",
    "    return row[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def granularity(df):\n",
    "    \n",
    "    #df['split_address'] = None\n",
    "    \n",
    "    pool = Pool(processes=round(len(df.index)/10000))\n",
    "\n",
    "    result_arr = []\n",
    "    \n",
    "    for result in tqdm.tqdm(pool.imap_unordered(granularity_helper, df.iterrows()),\n",
    "                            total=len(df.index)):\n",
    "        result_arr.append(result)\n",
    "                \n",
    "    df = pd.concat(result_arr, axis=1).transpose().sort_index()\n",
    "                \n",
    "    return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b7f148-8f6d-4434-bdb2-7c4c49f66695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmdf_loc = granularity(mdf_loc)\\nmdf_loc.to_feather(\"s3://mt5599/final/mentioned_loc.feather\")\\nmdf_loc_only = granularity(mdf_loc_only)\\nmdf_loc_only.to_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mdf_loc = granularity(mdf_loc)\n",
    "mdf_loc.to_feather(\"s3://mt5599/final/mentioned_loc.feather\")\n",
    "mdf_loc_only = granularity(mdf_loc_only)\n",
    "mdf_loc_only.to_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb78c51-a70a-4655-a33f-860bbc2fcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdf_loc\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"mdf_loc\")\n",
    "mdf_loc0 = pd.read_feather(\"s3://mt5599/final/mentioned_loc.feather\")\n",
    "#print(\"mdf_loc_only\")\n",
    "#mdf_loc_only0 = pd.read_feather(\"s3://mt5599/final/mentioned_loc_only.feather\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c529c-c134-4195-96e4-022fcea8aa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdf_loc = mdf_loc0[0:10000]\n",
    "#mdf_loc_only = mdf_loc0[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412081a-832f-4230-aab3-8ed20a66bd31",
   "metadata": {},
   "source": [
    "## Granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6f483-e2a8-4636-921c-fd11776229d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Figure \\ref{fig:mention-pie} shows the granularity of mentions (exluding None) and \"\\\n",
    "      \"Figure \\ref{fig:mention-pie-none} shows granularity including None. 'None'\"\\\n",
    "      \" corresponds to locations that did not have a match when parsed through Google's Geolocator API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec0111-4cb8-4f44-9bfd-725223be7db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating granularity without none\n",
    "\n",
    "loc_vc_1 = mdf_loc.granularity.value_counts(dropna=False).rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total_1 = sum(loc_vc_1.counts)\n",
    "#only_loc_vc_1 = mdf_loc_only.granularity.value_counts(dropna=False).rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total_1 = sum(only_loc_vc_1.counts)\n",
    "\n",
    "vc_1 = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": [],\n",
    "     #                    \"only_loc\": [],\n",
    "     #                    \"only_loc_counts\": [],\n",
    "     #                    \"only_loc_prop\": []})\n",
    "\n",
    "n = len(mdf_loc.granularity.unique())\n",
    "for i in range(n):\n",
    "    vc_1.loc[i, \"loc\"] = loc_vc_1.loc[i, \"unique_values\"]\n",
    "    vc_1.loc[i, \"loc_counts\"] = loc_vc_1.loc[i, \"counts\"]\n",
    "    vc_1.loc[i, \"loc_prop\"] = loc_vc_1.loc[i, \"counts\"] / loc_total_1\n",
    "    \n",
    " #   vc_1.loc[i, \"only_loc\"] = only_loc_vc_1.loc[i, \"unique_values\"]\n",
    " #   vc_1.loc[i, \"only_loc_counts\"] = only_loc_vc_1.loc[i, \"counts\"]\n",
    " #   vc_1.loc[i, \"only_loc_prop\"] = only_loc_vc_1.loc[i, \"counts\"] / only_loc_total_1\n",
    "\"\"\"    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\"\"\"\n",
    "vc_1.reset_index(inplace=True, drop=True)\n",
    "vc_1.loc[0, \"loc\"] = \"None\"\n",
    "#vc_1.loc[0, \"only_loc\"] = \"None\"\n",
    "vc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd0017-f888-45e8-a40d-d2929b5cff4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating granularity with none\n",
    "\n",
    "loc_vc_2 = mdf_loc.granularity.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total_2 = sum(loc_vc_2.counts)\n",
    "#only_loc_vc_2 = mdf_loc_only.granularity.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total_2 = sum(only_loc_vc_2.counts)\n",
    "\n",
    "vc_2 = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": [],\n",
    " #                        \"only_loc\": [],\n",
    " #                        \"only_loc_counts\": [],\n",
    " #                        \"only_loc_prop\": []})\n",
    "\n",
    "n = len(mdf_loc.granularity.unique())\n",
    "for i in range(n-1):\n",
    "    vc_2.loc[i, \"loc\"] = loc_vc_2.loc[i, \"unique_values\"]\n",
    "    vc_2.loc[i, \"loc_counts\"] = loc_vc_2.loc[i, \"counts\"]\n",
    "    vc_2.loc[i, \"loc_prop\"] = loc_vc_2.loc[i, \"counts\"] / loc_total_2\n",
    "    \n",
    "  #  vc_2.loc[i, \"only_loc\"] = only_loc_vc_2.loc[i, \"unique_values\"]\n",
    " #   vc_2.loc[i, \"only_loc_counts\"] = only_loc_vc_2.loc[i, \"counts\"]\n",
    " #   vc_2.loc[i, \"only_loc_prop\"] = only_loc_vc_2.loc[i, \"counts\"] / only_loc_total_2\n",
    "\"\"\"    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\"\"\"\n",
    "vc_2.reset_index(inplace=True, drop=True)\n",
    "vc_2.loc[0, \"loc\"] = \"None\"\n",
    "#vc_2.loc[0, \"only_loc\"] = \"None\"\n",
    "vc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8cf76-08dd-4107-a9b5-37d94704a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#define Seaborn \n",
    "#color palette to use\n",
    "colors = sns.color_palette('pastel')[0:n+1]\n",
    "\"\"\"\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, )\n",
    "\n",
    "#fig.suptitle('Vertically stacked subplots')\n",
    "ax1.pie(vc[\"loc_counts\"], labels = vc[\"loc\"], colors = colors, autopct='%.2f%%')\n",
    "ax1.title.set_text('Tweets with Mention')\n",
    "ax2.pie(vc[\"only_loc_counts\"], labels = vc[\"only_loc\"], colors = colors, autopct='%.2f%%')\n",
    "ax2.title.set_text('Tweets with Mention Only')\n",
    "\n",
    "#plt.savefig('mention_pie_none.png')\n",
    "fig.write_image(\"granularity.png\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872a6fd-ddfd-4dce-8414-8a154fb74ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,10))\n",
    "axs[0, 0].pie(vc_1[\"loc_counts\"], labels = vc_1[\"loc\"],\n",
    "          #    hatch = [\"..\", \"**\", \"oo\", \"OO\"],\n",
    "              colors = colors,\n",
    "              autopct='%.2f%%')\n",
    "axs[0, 0].set_title('Mention')\n",
    "axs[0, 1].pie(vc_1[\"only_loc_counts\"], labels = vc_1[\"only_loc\"],\n",
    "              colors = colors,\n",
    "         #     hatch = [\"..\", \"**\", \"oo\", \"OO\"],\n",
    "              autopct='%.2f%%')\n",
    "axs[0, 1].set_title('Mention Only')\n",
    "\n",
    "#axs[1, 0].pie(vc_2[\"loc_counts\"], labels = vc_2[\"loc\"],\n",
    "       #       hatch = [\"O.\", \"..\", \"**\", \"oo\", \"OO\"],\n",
    "              colors = colors,\n",
    "              autopct='%.2f%%')\n",
    "#axs[1, 0].set_title('Mention with None')\n",
    "#axs[1, 1].pie(vc_2[\"only_loc_counts\"], labels = vc_2[\"only_loc\"],\n",
    "       #       hatch = [\"O.\", \"..\", \"**\", \"oo\", \"OO\"],\n",
    "              colors = colors,\n",
    "              autopct='%.2f%%')\n",
    "#axs[1, 1].set_title('Mention Only with None')\n",
    "\n",
    "#for ax in axs.flat:\n",
    "#    ax.set(xlabel='', ylabel='y-label')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "fig.savefig(\"granularity.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2e305-c8ce-4fe1-9aea-ab90e9e108d7",
   "metadata": {},
   "source": [
    "# Distribution of Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5976cb5-3939-4153-a525-5f3f6521e82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = mdf_loc.drop_duplicates([\"id\"])\n",
    "#t2 = mdf_loc_only.drop_duplicates([\"id\"])\n",
    "\n",
    "print(\"Table ___ shows the mentioned locations by country for mentioned locations\"\\\n",
    "      \"(compared to tweets that had a mention and not a geotag).\" \\\n",
    "      \"The total number of tweets with mentions was \", t1.shape[0] ,\n",
    "   #   \" and with\"\\\n",
    "   #   \"mentions only was \", t2.shape[0] ,\n",
    "      \". Of those, \", t1[pd.isnull(t1.gmaps_address)].shape[0] ,\" and \",\n",
    "   #   t2[pd.isnull(t2.gmaps_address)].shape[0] ,\n",
    "      \" were missing values (respectively) due to geoparser not\"\\\n",
    "      \"having found a match or having been discarded.\"\\\n",
    "      \"This summary was generated by removing the missing values (which corresponded to\",\n",
    "      t1[pd.isnull(t1.gmaps_address)].shape[0] / t1.shape[0],\n",
    "      \" and \",\n",
    "   #   t2[pd.isnull(t2.gmaps_address)].shape[0] / t2.shape[0],\n",
    "      \" of their total numbers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27735481-f3dd-4226-803f-14d9f907b468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating countries by tweet number (geotags vs no geotags)\n",
    "\n",
    "loc_vc = mdf_loc.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#only_loc_vc = mdf_loc_only.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(only_loc_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": [],\n",
    "                         \"only_loc\": [],\n",
    "                         \"only_loc_counts\": [],\n",
    "                         \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    " #   vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    " #   vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    " #   vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "\n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eae3f-fde7-4600-85eb-f7b46be4f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For each user, the countries they mentioned over the course of the relevant time period were found.\")\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "user_df = mdf_loc.groupby(['username'])[\"country\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_df.countries.str.len()\n",
    "\n",
    "print()\n",
    "print(\"The mean number of mentioned countries was \", np.mean(countries_per_user),\n",
    "     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_df.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows their breakdown.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b394f-4246-4068-9f2e-e0a74dc04bb4",
   "metadata": {},
   "source": [
    "# Distribution of Places in Argentina, Brazil and USA\n",
    "\n",
    "See what works better - city/town or subcountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f8514-ebec-4bb4-8c82-5733a589413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGENTINA - tweets\n",
    "\n",
    "ar = mdf_loc[mdf_loc.country == \"Argentina\"]\n",
    "\n",
    "loc_vc = ar[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#loc_vc = ar[\"subcountry\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only =mdf_loc[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = mdf_loc_only.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e92c7a-dc62-491e-8d34-5fac25e8d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRAZIL - tweets\n",
    "\n",
    "ar = mdf_loc[mdf_loc.country == \"Brazil\"]\n",
    "\n",
    "loc_vc = ar[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#loc_vc = ar[\"subcountry\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only =mdf_loc[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = mdf_loc_only.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639517c9-f293-468e-9690-380ba80eecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US - tweets\n",
    "\n",
    "ar = mdf_loc[mdf_loc.country == \"USA\"]\n",
    "\n",
    "loc_vc = ar[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#loc_vc = ar[\"subcountry\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only =mdf_loc[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = mdf_loc_only.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73d9db-ad67-493d-a0e3-7d63bc3964df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGENTINA MENTIONS - by user\n",
    "\n",
    "ar = mdf_loc[mdf_loc.country == \"Argentina\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = ar.groupby(['username'])[\"city/town\"].unique().reset_index(name='countries')\n",
    "#user_ar = ar.groupby(['username'])[\"subcountry\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d232d11-5fcc-4d73-b5f1-f84a3507c469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f035b-a0c0-4337-b466-b8b9ca45f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRAZIL MENTIONS - by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef77ab-0874-4dc2-8fcc-9c3d9037f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US MENTIONS - by user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e84b9-c1f1-4ffa-ae01-6fcea5bb5efe",
   "metadata": {},
   "source": [
    "## Epi weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b78a2d-89c2-4810-bbaf-f9d2b5166644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - tweets\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_20 = mdf_loc.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#top_20 = top_20.loc[,0:20]\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "top_20\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = mdf_loc[mdf_loc.country.isin(top_20)]\n",
    "loc_p = loc_p.groupby(['country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(mdf_loc.country.unique(), mdf_loc.epi_week.unique())), columns=['country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "#only_loc_p = df_loc[df_loc.country.isin(top_20)]\n",
    "#only_loc_p = only_loc_p.groupby(['country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "# plot pie chart\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Tweets\",\n",
    "                     \"country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cde43-432b-4471-9075-f60e0d5f1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - users\n",
    "\n",
    "import itertools\n",
    "\n",
    "cepi = mdf_loc.drop_duplicates([\"username\", \"country\", \"epi_week\"])\n",
    "cepi = cepi.groupby(['country', \"epi_week\"])[\"username\"].count().reset_index(name='counts')\n",
    "cepi = cepi.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_20 = cepi.country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = cepi[cepi.country.isin(top_20)]\n",
    "#loc_p = loc_p.groupby(['country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "#loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(cepi.country.unique(), cepi.epi_week.unique())), columns=['country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Users in Country\",\n",
    "                     \"country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6873fee-4b5f-48b3-bd71-fbd80ba27dec",
   "metadata": {},
   "source": [
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap\n",
    "# gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c458e-981e-4bd0-8c45-427111ebee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ like above code, but for people:\n",
    "\n",
    "mdf_loc1 = mdf_loc[mdf_loc.country == \"Argentina\"]\n",
    "#mdf_loc_only1= mdf_loc_only[mdf_loc_only.country == \"Argentina\"]\n",
    "\n",
    "#loc_p = mdf_loc1.groupby(['city/town', \"epi_week\"])['id'].count().reset_index(name='counts')\n",
    "loc_p = mdf_loc1[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_p = mdf_loc_only1.groupby(['city/town', \"epi_week\"])['id'].count().reset_index(name='counts')\n",
    "\n",
    "# plot pie chart\n",
    "loc_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0c1ce-cc62-49d5-bd8d-eb5964fc28b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating countries with Argnetina (geotags vs no geotags)\n",
    "\n",
    "# Investigating granularity (geotags vs no geotags)\n",
    "\n",
    "mdf_loc1 = mdf_loc[mdf_loc.country == \"Argentina\"]\n",
    "loc_vc = mdf_loc1[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#mdf_loc_only1= mdf_loc_only[mdf_loc_only.country == \"Argentina\"]\n",
    "#only_loc_vc = mdf_loc_only1[\"city/town\"].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(only_loc_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": [],\n",
    "    #                     \"only_loc\": [],\n",
    "    #                     \"only_loc_counts\": [],\n",
    "    #                     \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    " #   vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    " #   vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    " #   vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "   \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "vc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9defd-3f8b-4948-9cc2-9371af877d4b",
   "metadata": {},
   "source": [
    "### Epi Weeks\n",
    "\n",
    "Where are people during each Epi week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cbf22-cf7c-486d-9abd-990ce2a4fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_p = mdf_loc.groupby(['country', \"epi_week\"])['username'].count().reset_index(name='counts')\n",
    "#only_loc_p = mdf_loc_only.groupby(['country', \"epi_week\"])['username'].count().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8921cb7-69ff-454f-b26c-588edb4d2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of people in each country over time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size and style\n",
    "sns.set(rc={'figure.figsize':(10,7)})\n",
    "sns.set_style(style={'axes.edgecolor': 'black', 'axes.facecolor': 'white'})\n",
    "\n",
    "sns.lineplot(data=loc_p, x=\"epi_week\", y=\"counts\", hue=\"country\")\n",
    "\n",
    "# Rename axis labels\n",
    "plt.xlabel('Epi Week / Date')\n",
    "plt.ylabel('Number of Users')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.savefig('distance_dup_frequency.png')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5d.16xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
