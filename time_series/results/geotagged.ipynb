{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ce254e-82d5-4216-99d1-c25aa3492ddd",
   "metadata": {},
   "source": [
    "# Geotagged Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de8662f-0057-4a95-accb-8281753ab4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: epiweeks in /opt/conda/lib/python3.7/site-packages (2.1.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kaleido in /opt/conda/lib/python3.7/site-packages (0.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install epiweeks\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036ea77b-39dd-4e59-9020-3ed261785cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool # multithreading\n",
    "from epiweeks import Week\n",
    "\n",
    "# granularity\n",
    "def granularity_helper(row):\n",
    "    \n",
    "    s = row[1][\"gmaps_address\"]\n",
    "    \n",
    "    week = Week.fromdate(row[1][\"DateTime\"], system=\"iso\")\n",
    "    row[1][\"epi_week\"] = week.cdcformat()\n",
    "    return row[1]\n",
    "\n",
    "\n",
    "\n",
    "def granularity(df):\n",
    "    \n",
    "    pool = Pool(processes=round(len(df.index)/10000))\n",
    "\n",
    "    result_arr = []\n",
    "    \n",
    "\n",
    "    for result in tqdm.tqdm(pool.imap_unordered(granularity_helper, df.iterrows()),\n",
    "                            total=len(df.index)):\n",
    "        result_arr.append(result)\n",
    "    \n",
    "\n",
    "    df = pd.concat(result_arr, axis=1).transpose().sort_index()\n",
    "\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60be6f-0b41-4752-ab52-c9f304c0e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737192/737192 [04:46<00:00, 2568.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 721111/721111 [04:48<00:00, 2497.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 689594/689594 [04:40<00:00, 2457.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716782/716782 [04:42<00:00, 2535.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625751/625751 [04:05<00:00, 2550.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653427/653427 [04:24<00:00, 2472.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654236/654236 [04:33<00:00, 2393.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 707571/707571 [04:50<00:00, 2432.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 671800/671800 [04:41<00:00, 2387.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629647/629647 [04:24<00:00, 2383.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887220/887220 [06:10<00:00, 2395.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587758/587758 [04:02<00:00, 2418.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628901/628901 [04:22<00:00, 2396.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 623732/623732 [04:22<00:00, 2376.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572787/572787 [04:04<00:00, 2340.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 707122/707122 [05:04<00:00, 2321.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547599/547599 [03:52<00:00, 2353.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557465/557465 [04:02<00:00, 2297.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615687/615687 [04:31<00:00, 2271.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538975/538975 [03:58<00:00, 2264.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575763/575763 [04:11<00:00, 2287.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536904/536904 [03:55<00:00, 2276.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538596/538596 [03:59<00:00, 2248.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 544151/544151 [04:03<00:00, 2231.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587018/587018 [04:28<00:00, 2188.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550325/550325 [04:06<00:00, 2235.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534312/534312 [03:57<00:00, 2247.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510720/510720 [03:53<00:00, 2184.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531483/531483 [04:01<00:00, 2199.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579238/579238 [04:20<00:00, 2220.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511820/511820 [03:53<00:00, 2189.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  31\n"
     ]
    }
   ],
   "source": [
    "################ GEOTAGGED ONLY ##########################\n",
    "\n",
    "# get distances column from all datasets\n",
    "# first determine if there are any duplicate tweet ids in there\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "for i in range(0, 112):\n",
    "    print(\"starting \", i)\n",
    "    df = pd.read_feather(\"s3://mt5599/final/processed_tweets_\" + str(i) + \".feather\")\n",
    "\n",
    "    # keep tweets with no distances and NER mention\n",
    "    df = df[pd.notnull(df.place_full_name)]\n",
    "    df = granularity(df)\n",
    "    df = df.reset_index()\n",
    "    df.to_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9bf71-84d9-4b5c-a2c0-8ea5f3a5eeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f10113-474a-4fa2-8011-cc6bc5eb64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining into one dataset\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    df = pd.read_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(drop=True).reset_index()\n",
    "df.to_feather(\"s3://mt5599/final/geotagged.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2978574-fd7a-4273-8c8e-5e0bb0977a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(0, 112)):\n",
    "    df = pd.read_feather(\"s3://mt5599/dissertation/geotagged_\" + str(i) + \".feather\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(drop=True).reset_index()\n",
    "df.to_feather(\"s3://mt5599/final/geotagged.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdb808-d041-44cd-944c-392365bb04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather(\"s3://mt5599/final/geotagged.feather\")\n",
    "print(\"read\")\n",
    "df = df.drop_duplicates(\"id\").drop([\"level_0\", \"index\"], axis=1)\n",
    "print(\"cleaning\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d750666-3cb8-4bb9-b719-d1140ed082a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Investigating top 20 most geotagged countries and their frequencies\n",
    "\n",
    "loc_vc = df.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged countries,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c06716-a652-48ee-9dea-566abc7873f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For each user, the countries they geotagged over the course of the relevant time period were found.\")\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "user_df = df.groupby(['username'])[\"place_country\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_df.countries.str.len()\n",
    "\n",
    "print()\n",
    "print(\"The mean number of geotagged countries was \", np.mean(countries_per_user),\n",
    "     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_df.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows their breakdown.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bce112-631c-4ec5-8c2f-b39a930d311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ARGENTINA - tweets\n",
    "\n",
    "ar = df[df.place_country_code == \"AR\"]\n",
    "\n",
    "loc_vc = ar.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8a2c3-13c7-4c85-9cbf-8bd8e4bde21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNITED STATES - tweets\n",
    "\n",
    "us = df[df.place_country_code == \"US\"]\n",
    "\n",
    "loc_vc = us.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc7272-7b6f-4d77-8e63-4b859536b295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BRAZIL - tweets\n",
    "\n",
    "br = df[df.place_country_code == \"BR\"]\n",
    "\n",
    "loc_vc = br.place_name.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "loc_total = sum(loc_vc.counts)\n",
    "#df_only = df[df.ner_type != \"LOC\"]\n",
    "#only_loc_vc = df_only.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#only_loc_total = sum(loc_only_vc.counts)\n",
    "\n",
    "vc = pd.DataFrame(data = {\"loc\": [],\n",
    "                          \"loc_counts\": [],\n",
    "                          \"loc_prop\": []})\n",
    "                    #     \"only_loc\": [],\n",
    "                   #      \"only_loc_counts\": [],\n",
    "                   #      \"only_loc_prop\": []})\n",
    "\n",
    "n = 20\n",
    "for i in range(n):\n",
    "    vc.loc[i, \"loc\"] = loc_vc.loc[i, \"unique_values\"]\n",
    "    vc.loc[i, \"loc_counts\"] = loc_vc.loc[i, \"counts\"]\n",
    "    vc.loc[i, \"loc_prop\"] = loc_vc.loc[i, \"counts\"] / loc_total\n",
    "    \n",
    "  #  vc.loc[i, \"only_loc\"] = only_loc_vc.loc[i, \"unique_values\"]\n",
    "  #  vc.loc[i, \"only_loc_counts\"] = only_loc_vc.loc[i, \"counts\"]\n",
    "  #  vc.loc[i, \"only_loc_prop\"] = only_loc_vc.loc[i, \"counts\"] / only_loc_total\n",
    "    \n",
    "vc.loc[n+1, \"loc\"] = \"Other\"\n",
    "vc.loc[n+1, \"loc_counts\"] = loc_total - sum(loc_vc.loc[0:n, \"counts\"])\n",
    "vc.loc[n+1, \"loc_prop\"] = vc.loc[n+1, \"loc_counts\"] / loc_total\n",
    "#vc.loc[n+1, \"only_loc\"] = \"Other\"\n",
    "#vc.loc[n+1, \"only_loc_counts\"] = only_loc_total - sum(only_loc_vc.loc[0:n, \"counts\"])\n",
    "#vc.loc[n+1, \"only_loc_prop\"] = vc.loc[n+1, \"only_loc_counts\"] / only_loc_total\n",
    "\n",
    "vc.reset_index(inplace=True, drop=True)\n",
    "#vc.drop(0, axis=0, inplace=True)\n",
    "print(\"Table \\ref{tab:geotagged-locations} shows the number of top 20 most geotagged locations within Argentina,\",\n",
    "      \"with 'Other' denoting other countries (does not include un-geotagged tweets).\")\n",
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b1846-9562-4839-8d75-4d0c65d45c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ARGENTINA GEOTAGS - by user\n",
    "\n",
    "ar = df[df.place_country_code == \"AR\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = ar.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9ab54-2dca-4dfe-b7fb-1f7ee0f10397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"There were a total of \", ar.shape[0],\" geotagged tweets from Argentina.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93943bfe-2e12-4279-9d2d-e4f5e2e71b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNITED STATES GEOTAGS - by user\n",
    "\n",
    "us = df[df.place_country_code == \"US\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = us.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6a9fe-074e-4d8b-86b0-6cc036ea40d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BRAZIL GEOTAGS - by user\n",
    "\n",
    "br = df[df.place_country_code == \"BR\"]\n",
    "\n",
    "import itertools\n",
    "\n",
    "user_ar = br.groupby(['username'])[\"place_name\"].unique().reset_index(name='countries')\n",
    "\n",
    "# number of countries per user\n",
    "countries_per_user = user_ar.countries.str.len()\n",
    "\n",
    "#print()\n",
    "#print(\"The mean number of geotagged placenames in Argentina was \", np.mean(countries_per_user),\n",
    "#     \"and the median was \", np.median(countries_per_user))\n",
    "\n",
    "user_countries = user_ar.countries.tolist()\n",
    "all_countries = pd.DataFrame({\"country\":list(itertools.chain.from_iterable(user_countries))})\n",
    "temp = pd.DataFrame(all_countries.country.value_counts())\n",
    "\n",
    "print(\"Table \\ref{tab:geotagged-locations-users} shows the breakdown of top 20 most common geotags\",\n",
    "      \"within Argentina aggregated by user to avoid the problem of varying tweeting frequencies.\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f14903-67c8-47c5-8ac2-6afa77070842",
   "metadata": {},
   "source": [
    "## Epi Weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b434b4-3ce5-4967-94e2-52dcbaee64c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - tweets\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_20 = df.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#top_20 = top_20.loc[,0:20]\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "top_20\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = df[df.place_country.isin(top_20)]\n",
    "loc_p = loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(df.place_country.unique(), df.epi_week.unique())), columns=['place_country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"place_country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "#only_loc_p = df_loc[df_loc.place_country.isin(top_20)]\n",
    "#only_loc_p = only_loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "# plot pie chart\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"place_country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Tweets\",\n",
    "                     \"place_country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdce71-c59e-4b98-b4b0-d7e276d39204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries over Epi Weeks - users\n",
    "\n",
    "import itertools\n",
    "\n",
    "cepi = df.drop_duplicates([\"username\", \"place_country\", \"epi_week\"])\n",
    "cepi = cepi.groupby(['place_country', \"epi_week\"])[\"username\"].count().reset_index(name='counts')\n",
    "cepi = cepi.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "top_20 = cepi.place_country.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "top_20 = top_20.loc[0:20, \"unique_values\"].tolist()\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "loc_p = cepi[cepi.place_country.isin(top_20)]\n",
    "#loc_p = loc_p.groupby(['place_country', \"epi_week\"])[\"id\"].count().reset_index(name='counts')\n",
    "#loc_p = loc_p.sort_values([\"epi_week\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "loc_temp = pd.DataFrame(list(product(cepi.place_country.unique(), df.epi_week.unique())), columns=['place_country', 'epi_week'])\n",
    "loc_temp[\"counts\"] = 0\n",
    "\n",
    "loc_p = pd.concat([loc_p, loc_temp]).drop_duplicates([\"place_country\", \"epi_week\"])#, ignore_index=True)\n",
    "loc_p = loc_p.sort_values([\"epi_week\"], ascending=True)\n",
    "\n",
    "\n",
    "fig = px.line(loc_p, x=\"epi_week\", y=\"counts\", color=\"place_country\",\n",
    "                 labels={\n",
    "                     \"epi_week\": \"Epi Week\",\n",
    "                     \"counts\": \"Number of Users in Country\",\n",
    "                     \"place_country\": \"Country\"})\n",
    "\n",
    "fig.write_image(\"geotagged_users_country_epi_week.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94181c31-c63a-4bf8-98af-08fa4d4799ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lower value counts of Place Object?"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5d.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
