{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6297567-fdd1-4af6-9443-0043f29a09c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ccafe65-90e4-4981-a837-e94ccea001a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c725d6e-359e-4549-a144-86d1a33f839d",
   "metadata": {},
   "source": [
    "# Week 8 Progress Report\n",
    "\n",
    "The purpose of this report is to record the progress on the MT5599 Project.\n",
    "\n",
    "The main milestones of the project so far are:\n",
    "\n",
    "- Tweets from Argentinian residents and visitors from 2016 & 2020 have been collected\n",
    "- Dataset from PAHO WHO of dengue cases from 2016 to 2021 has been found\n",
    "- There is a second dataset of all PAHO dengue cases from a similar period exists and I have it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cc561-8428-4623-aa81-6778a17f0c6a",
   "metadata": {},
   "source": [
    "## Tweets from Argentina\n",
    "\n",
    "### Collecting Tweets\n",
    "\n",
    "The original proposed method for collecting tweets was:\n",
    "\n",
    "1. Choose 2-3 months from each epidemic year (2016, 2020) and collect all tweets that are geotagged \"Argentina\"\n",
    "2. Identify those handles whose user location is Argentina for each 2016, 2020 (residents)\n",
    "3. Collect all their tweets from the affected year\n",
    "\n",
    "However, it was found that there were very handles collected this way (a few hundred), which was not at all representative of Argentina's roughly 5.9 million Twitter active users.\n",
    "\n",
    "This is why the new method became:\n",
    "\n",
    "1. Collect all tweets from near Argentina during selected periods (2015-10-01 to 2016-11-01) and (2019-06-01 and 2020-07-01) _See `scraper_handles.py` for this script_\n",
    "2. If person had an Argentinian city/town/province in their user information, they were labelled as a resident. If they had a tweet with country tag AR (Argentina), they were labelled as a visitor. There is overlap, however in both years the number of residents is significantly lower than the number of visitors. The duplicates were removed in later analysis. _See `extracting_handles.py` and `simpler_getting_argentinian_residents.ipynb`_\n",
    "3. All the tweets from the above collected handles were scraped during the selected periods. This yielded roughly ___ for 2016 and ___ for 2020. _See `EDA.ipynb` for insights_\n",
    "\n",
    "### Cleaning Tweets\n",
    "\n",
    "Many tweets have emojis, hashtags, links, etc. These were cleaned using `cleaning_tweets.py`.\n",
    "\n",
    "### Processing Tweets\n",
    "\n",
    "The purpose of the tweets is to tell us:\n",
    "\n",
    "- where people were at all times of the year\n",
    "- what their purpose of travel was\n",
    "- whether they mentioned dengue or other relevant topics\n",
    "\n",
    "To tell us where people were at all times of the year, we can use the coordinates data (which not all tweets have) and multilingual NER (by extracting location tags). \n",
    "\n",
    "\n",
    "To tell us what the purpose of travel was, we can use topic modelling.\n",
    "\n",
    "To see if they mentioned dengue, we can use a keyword search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf48b40-2e44-4d5a-b7f9-aa69243bff87",
   "metadata": {},
   "source": [
    "## Dengue Cases\n",
    "\n",
    "Since the goal is to see how transmission is affected by travel, we need information about dengue cases.\n",
    "\n",
    "There exist two datasets:\n",
    "\n",
    "- From https://github.com/sirmammingtonham/vector-borne-disease-analytics which is scraped from ProMed Mail. The scraper created in this research is supposed to scrape all dengue cases, however it only scrapes data from the PAHO region. When I tried to run the scraper myself (not just use the dataset in repository), it seems there is something wrong with the .yaml environment. The two solutions are either to fix this myself, or manually collect the rest of the data from ProMed.\n",
    "- From Dr Rachel Sippy. This contains dengue cases by week from PAHO region since 2014 and by month from Europe since 2008.\n",
    "\n",
    "The issue is that we don't have worldwide data. We know that dengue cases can be high in Asia, and people travelling between Argentina and south Asia could be reintroducing dengue to Argentina, but we wouldn't know this because we are missing this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628af08-4fb9-4b15-b19e-9b7582ab0429",
   "metadata": {},
   "source": [
    "## Plan Going Forward\n",
    "\n",
    "Since 6 weeks have already been spent on data collection, it may be worthwile creating a complete pipeline with NER, topic modelling and other NLP techniques.\n",
    "\n",
    "The plan is as follows:\n",
    "\n",
    "__Week 8:__ Create NER script. Create initial map plots with existing dengue case data and tweets location data.\n",
    "\n",
    "__Week 9:__ Create Topic Modelling pipeline.\n",
    "\n",
    "__Week 10:__ Clean entire pipeline and repository.\n",
    "\n",
    "__Week 11:__ Collect missing dengue data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae9c04-221a-4ef8-a146-2bf74c502161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
